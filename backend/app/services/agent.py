"""
Personal Agent with LLM-based Intent Recognition
Hybrid Memory Architecture + Smart Intent Understanding
"""
from typing import Sequence, Optional
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage
from langchain_core.tools import tool
from langchain_anthropic import ChatAnthropic
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from datetime import datetime
from motor.motor_asyncio import AsyncIOMotorClient
import json
from app.core.config import settings


# ============ Tools ============
@tool
def get_current_time() -> str:
    """èŽ·å–å½“å‰æ—¶é—´"""
    now = datetime.now()
    return now.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")


@tool
def calculate(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼ï¼Œä¾‹å¦‚: 2 + 2, 10 * 5"""
    try:
        result = eval(expression)
        return f"è®¡ç®—ç»“æžœ: {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {str(e)}"


# ============ Memory Service ============
class MongoMemoryService:
    """MongoDB-based memory service"""

    def __init__(self, connection_string: str = "mongodb://localhost:27017"):
        self.client: Optional[AsyncIOMotorClient] = None
        self.connection_string = connection_string
        self._db = None
        self._conversations = None
        self._long_term = None

    async def _get_collections(self):
        if self._conversations is None:
            self.client = AsyncIOMotorClient(self.connection_string)
            self._db = self.client["agent_memory"]
            self._conversations = self._db["conversations"]
            self._long_term = self._db["long_term_memory"]

            await self._conversations.create_index([("thread_id", 1)])
            await self._conversations.create_index([("timestamp", -1)])
            await self._long_term.create_index([("thread_id", 1)])
            await self._long_term.create_index([("importance", -1)])

        return self._conversations, self._long_term

    async def save_conversation(self, thread_id: str, user_message: str, assistant_response: str):
        try:
            conversations, _ = await self._get_collections()
            doc = {
                "thread_id": thread_id,
                "user_message": user_message,
                "assistant_response": assistant_response,
                "timestamp": datetime.utcnow(),
            }
            await conversations.insert_one(doc)
        except Exception as e:
            print(f"Error saving conversation: {e}")

    async def get_conversation_history(self, thread_id: str, limit: int = 20) -> list[dict]:
        try:
            conversations, _ = await self._get_collections()
            cursor = conversations.find({"thread_id": thread_id}).sort("timestamp", -1).limit(limit)
            docs = await cursor.to_list(length=limit)
            return list(reversed(docs))
        except Exception as e:
            print(f"Error getting conversation history: {e}")
            return []

    async def save_fact(self, thread_id: str, fact_type: str, content: str, importance: float = 0.5):
        try:
            _, long_term = await self._get_collections()
            doc = {
                "thread_id": thread_id,
                "fact_type": fact_type,
                "content": content,
                "importance": importance,
                "timestamp": datetime.utcnow(),
            }
            await long_term.update_one(
                {"thread_id": thread_id, "fact_type": fact_type, "content": content},
                {"$set": doc},
                upsert=True
            )
        except Exception as e:
            print(f"Error saving fact: {e}")

    async def get_facts(self, thread_id: str, limit: int = 10) -> list[str]:
        try:
            _, long_term = await self._get_collections()
            cursor = long_term.find({"thread_id": thread_id}).sort("importance", -1).limit(limit)
            docs = await cursor.to_list(length=limit)
            return [doc["content"] for doc in docs]
        except Exception as e:
            print(f"Error getting facts: {e}")
            return []

    async def delete_fact(self, thread_id: str, content: str) -> bool:
        try:
            _, long_term = await self._get_collections()
            result = await long_term.delete_many({
                "thread_id": thread_id,
                "content": {"$regex": content, "$options": "i"}
            })
            return result.deleted_count > 0
        except Exception as e:
            print(f"Error deleting fact: {e}")
            return False

    async def clear_all_facts(self, thread_id: str) -> int:
        try:
            _, long_term = await self._get_collections()
            result = await long_term.delete_many({"thread_id": thread_id})
            return result.deleted_count
        except Exception as e:
            print(f"Error clearing facts: {e}")
            return 0

    async def list_all_facts(self, thread_id: str) -> list[dict]:
        try:
            _, long_term = await self._get_collections()
            cursor = long_term.find({"thread_id": thread_id}).sort("importance", -1)
            docs = await cursor.to_list(length=None)
            return docs
        except Exception as e:
            print(f"Error listing facts: {e}")
            return []

    async def close(self):
        if self.client:
            self.client.close()
            self._conversations = None
            self._long_term = None


# ============ LLM Intent Recognizer ============
class IntentRecognizer:
    """Use LLM to intelligently recognize user intent"""

    def __init__(self):
        anthropic_kwargs = {
            "api_key": settings.anthropic_api_key,
            "model": settings.model_name,
            "temperature": 0.0  # Low temperature for consistent classification
        }
        if settings.anthropic_base_url:
            anthropic_kwargs["base_url"] = settings.anthropic_base_url

        self.llm = ChatAnthropic(**anthropic_kwargs)

        self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ„å›¾è¯†åˆ«åŠ©æ‰‹ã€‚åˆ†æžç”¨æˆ·çš„è¾“å…¥ï¼Œåˆ¤æ–­ä»–ä»¬çš„æ„å›¾ã€‚

å¯èƒ½çš„æ„å›¾ç±»åž‹ï¼š
1. chat - æ­£å¸¸å¯¹è¯
2. delete_memory - åˆ é™¤ç‰¹å®šè®°å¿†
3. view_memories - æŸ¥çœ‹æ‰€æœ‰è®°å¿†
4. clear_memories - æ¸…ç©ºæ‰€æœ‰è®°å¿†

è¯·ä»¥ JSON æ ¼å¼è¿”å›žï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{
  "intent": "æ„å›¾ç±»åž‹",
  "confidence": 0.0-1.0 ä¹‹é—´çš„ç½®ä¿¡åº¦,
  "extracted_info": {
    "query": "è¦åˆ é™¤/æŸ¥è¯¢çš„å…³é”®è¯ï¼ˆå¦‚æžœæœ‰ï¼‰",
    "reason": "åˆ¤æ–­ç†ç”±"
  }
}

ç¤ºä¾‹ï¼š
è¾“å…¥: "å¿˜è®°æˆ‘å–œæ¬¢å’–å•¡"
è¾“å‡º: {"intent": "delete_memory", "confidence": 0.95, "extracted_info": {"query": "æˆ‘å–œæ¬¢å’–å•¡", "reason": "ç”¨æˆ·æ˜Žç¡®è¦æ±‚å¿˜è®°æŸä¸ªåå¥½"}}

è¾“å…¥: "ä½ éƒ½çŸ¥é“ä»€ä¹ˆ"
è¾“å‡º: {"intent": "view_memories", "confidence": 0.9, "extracted_info": {"query": "", "reason": "ç”¨æˆ·æƒ³æŸ¥çœ‹AIçŸ¥é“çš„ä¿¡æ¯"}}

è¾“å…¥: "åˆ«è®°ç€æˆ‘å–œæ¬¢åƒè¾£"
è¾“å‡º: {"intent": "delete_memory", "confidence": 0.92, "extracted_info": {"query": "å–œæ¬¢åƒè¾£", "reason": "ç”¨æˆ·è¦æ±‚ä¸è¦è®°ä½è¿™ä¸ªåå¥½"}}

è¾“å…¥: "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€Žä¹ˆæ ·"
è¾“å‡º: {"intent": "chat", "confidence": 0.98, "extracted_info": {"query": "", "reason": "æ™®é€šé—®å€™å’Œé—²èŠ"}}

åªè¿”å›ž JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

    async def recognize_intent(self, user_message: str) -> dict:
        """Recognize user intent using LLM"""
        try:
            messages = [
                SystemMessage(content=self.system_prompt),
                HumanMessage(content=user_message)
            ]

            response = await self.llm.ainvoke(messages)
            result = response.content

            # Parse JSON response
            if "```json" in result:
                result = result.split("```json")[1].split("```")[0].strip()
            elif "```" in result:
                result = result.split("```")[1].split("```")[0].strip()

            intent_data = json.loads(result)

            # Validate intent
            valid_intents = ["chat", "delete_memory", "view_memories", "clear_memories"]
            if intent_data.get("intent") not in valid_intents:
                intent_data["intent"] = "chat"
                intent_data["extracted_info"] = {
                    "query": "",
                    "reason": "æ— æ³•è¯†åˆ«çš„æ„å›¾ï¼Œä½œä¸ºæ™®é€šå¯¹è¯å¤„ç†"
                }

            return intent_data

        except Exception as e:
            print(f"LLM intent recognition failed: {e}, using keyword fallback")
            return self._keyword_fallback(user_message)

    def _keyword_fallback(self, user_message: str) -> dict:
        """Fallback to keyword matching if LLM fails"""
        message_lower = user_message.lower()

        if any(word in message_lower for word in ["å¿˜è®°", "åˆ é™¤è®°å¿†", "ä¸è¦è®°ä½", "åˆ«è®°ç€"]):
            query = user_message
            for word in ["å¿˜è®°", "åˆ é™¤è®°å¿†", "ä¸è¦è®°ä½", "åˆ«è®°ç€"]:
                if word in message_lower:
                    query = user_message.split(word)[1].strip()
                    break

            return {
                "intent": "delete_memory",
                "confidence": 0.7,
                "extracted_info": {
                    "query": query,
                    "reason": "å…³é”®è¯åŒ¹é…è¯†åˆ«"
                }
            }

        if any(word in message_lower for word in ["æŸ¥çœ‹è®°å¿†", "è®°å¿†åˆ—è¡¨", "æ‰€æœ‰è®°å¿†", "ä½ çŸ¥é“ä»€ä¹ˆ", "ä½ éƒ½è®°å¾—ä»€ä¹ˆ"]):
            return {
                "intent": "view_memories",
                "confidence": 0.7,
                "extracted_info": {
                    "query": "",
                    "reason": "å…³é”®è¯åŒ¹é…è¯†åˆ«"
                }
            }

        if any(word in message_lower for word in ["æ¸…ç©ºè®°å¿†", "åˆ é™¤æ‰€æœ‰è®°å¿†", "å…¨éƒ¨å¿˜è®°", "é‡ç½®è®°å¿†"]):
            return {
                "intent": "clear_memories",
                "confidence": 0.7,
                "extracted_info": {
                    "query": "",
                    "reason": "å…³é”®è¯åŒ¹é…è¯†åˆ«"
                }
            }

        return {
            "intent": "chat",
            "confidence": 0.6,
            "extracted_info": {
                "query": "",
                "reason": "é»˜è®¤å¯¹è¯æ„å›¾"
            }
        }


# ============ Agent Service ============
class AgentService:
    def __init__(self):
        # Initialize LLM (Anthropic-compatible for GLM-4.7)
        anthropic_kwargs = {
            "api_key": settings.anthropic_api_key,
            "model": settings.model_name,
            "temperature": 0.7
        }
        if settings.anthropic_base_url:
            anthropic_kwargs["base_url"] = settings.anthropic_base_url

        self.llm = ChatAnthropic(**anthropic_kwargs)

        # Store reference to memory service
        self.mongo_memory = MongoMemoryService(
            connection_string=settings.mongodb_connection_string
        )

        # Initialize intent recognizer
        self.intent_recognizer = IntentRecognizer()

        # Define tools
        self.tools = [get_current_time, calculate]

        # Layer 1: Short-term memory (in-memory checkpoint)
        self.checkpointer = MemorySaver()

        # Build LangGraph agent
        self.graph = create_react_agent(
            self.llm,
            self.tools,
            checkpointer=self.checkpointer
        )

    async def chat(self, message: str, conversation_id: str | None = None) -> tuple[str, str]:
        """Chat with the agent and return (response, conversation_id)."""
        config = {"configurable": {"thread_id": conversation_id or "default"}}
        thread_id = config["configurable"]["thread_id"]

        # Step 1: Use LLM to recognize intent
        intent_result = await self.intent_recognizer.recognize_intent(message)
        intent = intent_result.get("intent", "chat")
        confidence = intent_result.get("confidence", 0.5)
        extracted_info = intent_result.get("extracted_info", {})

        print(f"[DEBUG] Intent: {intent}, Confidence: {confidence}, Info: {extracted_info}")

        # Step 2: Handle memory management intents
        if intent == "delete_memory" and confidence > 0.7:
            query = extracted_info.get("query", message)
            deleted = await self.mongo_memory.delete_fact(thread_id, query)

            if deleted:
                return f"âœ… å·²åˆ é™¤å…³äºŽã€Œ{query}ã€çš„è®°å¿†", thread_id
            else:
                return f"âŒ æ²¡æœ‰æ‰¾åˆ°å…³äºŽã€Œ{query}ã€çš„è®°å¿†", thread_id

        if intent == "view_memories" and confidence > 0.7:
            facts = await self.mongo_memory.list_all_facts(thread_id)

            if not facts:
                return "ðŸ“ å½“å‰æ²¡æœ‰ä»»ä½•é•¿æœŸè®°å¿†", thread_id

            result = "ðŸ“ æˆ‘çš„è®°å¿†åˆ—è¡¨ï¼š\n\n"
            for i, fact in enumerate(facts, 1):
                result += f"{i}. **{fact['fact_type']}**\n"
                result += f"   {fact['content']}\n\n"

            return result.strip(), thread_id

        if intent == "clear_memories" and confidence > 0.7:
            count = await self.mongo_memory.clear_all_facts(thread_id)
            return f"âœ… å·²æ¸…ç©º {count} æ¡è®°å¿†", thread_id

        # Step 3: Normal conversation with memory enhancement
        facts = await self.mongo_memory.get_facts(thread_id)

        enhanced_message = message
        if facts:
            context = "\n".join([f"- {fact}" for fact in facts])
            enhanced_message = f"[ç”¨æˆ·èƒŒæ™¯ä¿¡æ¯]\n{context}\n\n[å½“å‰æ¶ˆæ¯]\n{message}"

        result = await self.graph.ainvoke(
            {"messages": [HumanMessage(content=enhanced_message)]},
            config=config
        )

        response_message = result["messages"][-1]
        response = response_message.content if hasattr(response_message, 'content') else str(response_message)

        await self._extract_and_save_facts(thread_id, message, response)
        await self.mongo_memory.save_conversation(thread_id, message, response)

        return response, thread_id

    async def _extract_and_save_facts(self, thread_id: str, user_message: str, assistant_response: str):
        """Extract and save important facts from conversation"""
        message_lower = user_message.lower()

        if "æˆ‘å«" in user_message or "æˆ‘æ˜¯" in user_message:
            if "æˆ‘å«" in user_message:
                name_part = user_message.split("æˆ‘å«")[1].strip()
                name = name_part.split()[0] if name_part else ""
                if name:
                    await self.mongo_memory.save_fact(thread_id, "name", f"ç”¨æˆ·å«{name}", importance=0.9)

        if "å–œæ¬¢" in user_message or "ä¸çˆ±" in user_message or "è®¨åŽŒ" in user_message:
            await self.mongo_memory.save_fact(thread_id, "preference", user_message, importance=0.7)

        if "è®°ä½" in user_message:
            await self.mongo_memory.save_fact(thread_id, "important_fact", user_message.replace("è®°ä½", "").strip(), importance=0.8)

    async def get_conversation_history(self, conversation_id: str | None = None) -> Sequence[BaseMessage]:
        config = {"configurable": {"thread_id": conversation_id or "default"}}
        state = self.graph.get_state(config)
        return state.values.get("messages", [])

    async def get_long_term_memory(self, conversation_id: str | None = None) -> list[str]:
        thread_id = conversation_id or "default"
        return await self.mongo_memory.get_facts(thread_id)

    async def close(self):
        await self.mongo_memory.close()


# Global agent instance
agent_service = AgentService()
